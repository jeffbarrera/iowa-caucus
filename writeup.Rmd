---
title: Challenge 1 &mdash; Iowa Caucus Predictions
author: Jeffrey Barrera & Jacob Fenton
output: 
  pdf_document:
    includes:
      in_header: header.tex
geometry: margin=1in
---

```{r, echo=FALSE, message=FALSE, include=TRUE}


##############
# IMPORT LIBRARIES
##############
library(glmnet)
library(Metrics)
library(lpridge)



##############
# CONSTANTS
##############
FIRST_POLLING_COL <- 12
VOTE_SHARE_COL <- 3
EP_BW <- 13 # found by testing MSE at different bandwidths in 2008 & 2012
EP_TRENDLINE <- 14 # found by testing MSE at bandwidth 13 and different trendlines in 2008 & 2012

# Jeff's working directory -- comment out and override
setwd("~//Documents/School/Stanford/Classes/Poli Sci 355B/Challenge 1/iowa-caucus")
#setwd("/Users/jfenton/github-whitelabel/polisci_355b/iowa-caucus")


##############
# LOAD AND CONVERT DATASETS
##############


# load results datasets
results2008 <- read.csv('results_2008.csv', stringsAsFactors=FALSE)
results2012 <- read.csv('results_2012.csv', stringsAsFactors=FALSE)

# df of 2016 candidates to predict
candidates_2016 <- data.frame(c('trump', 'cruz', 'rubio','bush','carson','christie','paul','huckabee','kasich'))
colnames(candidates_2016) <- c('candidate')


# load polling datasets
polls2008_iowa <- read.csv('pollster/data/cleaned_2008_iowa.csv', stringsAsFactors=FALSE)
polls2012_iowa <- read.csv('pollster/data/cleaned_2012_iowa.csv', stringsAsFactors=FALSE)
polls2016_iowa <- read.csv('pollster/data/cleaned_2016_iowa.csv', stringsAsFactors=FALSE)
polls2008_national <- read.csv('pollster/data/cleaned_2008_national.csv', stringsAsFactors=FALSE)
polls2012_national <- read.csv('pollster/data/cleaned_2012_national.csv', stringsAsFactors=FALSE)
polls2016_national <- read.csv('pollster/data/cleaned_2016_national.csv', stringsAsFactors=FALSE)


# clean polling data
cleanPolling <- function(df) {
	# replace '-' with NA
	df[df == '-'] <- NA

	# convert polling cols from char to numeric
	df[FIRST_POLLING_COL:ncol(df)] <- sapply(df[FIRST_POLLING_COL:ncol(df)], as.numeric)

	# convert pollster cols to factor
	df$POLLSTER <- as.factor(df$POLLSTER)

	# convert all colnames to lower case
	colnames(df) <- tolower(colnames(df))

	# subset to polling conducted before the caucus
	df <- df[df$days_to_caucus > 0,]

	return(df)
}

polls2008_iowa <- cleanPolling(polls2008_iowa)
polls2012_iowa <- cleanPolling(polls2012_iowa)
polls2016_iowa <- cleanPolling(polls2016_iowa)

polls2008_national <- cleanPolling(polls2008_national)
polls2012_national <- cleanPolling(polls2012_national)
polls2016_national <- cleanPolling(polls2016_national)


# clean results data
cleanResults <- function(df) {

	# convert all colnames to lower case
	colnames(df) <- tolower(colnames(df))

	# convert candidate names to lower case
	df$candidate <- tolower(df$candidate)

	return(df)
}

results2008 <- cleanResults(results2008)
results2012 <- cleanResults(results2012)



# loadMergeEpanechnikov
# ---------------------
# read, clean, and concat Epanechnikov predictions.
# return combined ep preds.
loadMergeEpanechnikov <- function() {

	# load CSVs
	ep_2008_2012_iowa <- read.csv("ep_interp_log_pred.csv", stringsAsFactors=FALSE)
	ep_2008_natl <- read.csv("2008_predictions_national.csv", stringsAsFactors=FALSE)
	ep_2012_natl <- read.csv("2012_predictions_national.csv", stringsAsFactors=FALSE)
	ep_2016_iowa <- read.csv("2016_predictions.csv", stringsAsFactors=FALSE)
	ep_2016_natl <- read.csv("2016_predictions_national.csv", stringsAsFactors=FALSE)

	# put all into a list
	ep_dfs <- list(ep_2008_2012_iowa, ep_2016_iowa, ep_2008_natl, ep_2012_natl, ep_2016_natl)

	# strip out unneeded columns to allow table merging
	ep_dfs_sub <- lapply(ep_dfs, subset, select=c(window_type, bw_value, year, scope, trendline_width, candidate, estimate))

	# merge all tables
	ep_preds <- do.call(rbind, ep_dfs_sub)

	# clean up candidate column
	ep_preds$candidate <- tolower(ep_preds$candidate)

	return(ep_preds)
}

ep_preds <- loadMergeEpanechnikov()




##############
# PREDICT POLL TRENDS
##############

#### SIMPLE AVERAGES

# averageLatestPollingCandidate
# --------------------------
# Given a candidate, polling_df, results_df, and time period, calculate a simple average of the vote shares
averageLatestPollingCandidate <- function(candidate, polling_df, results_df, time_period, plot) {

	# hack == return zero if bachmann
	if(candidate == "bachmann") {
		return(0)
	} else {

		# subset polling_df to the specificed candidate and time period
		polls <- subset(polling_df, days_to_caucus < time_period & days_to_caucus > 1, select=candidate)
		poll_mean <- mean(polls[,1])
		poll_sd <- sd(polls[,1])

		return(poll_mean)
	}
}

#### LINEAR MODELS

# linearExtrapolateCandidate
# --------------------------
# Given a candidate, polling_df, and results_df, generate a simple linear model regressing vote shares over time
# return predicted vote share on election day

linearExtrapolateCandidate <- function(candidate, polling_df, results_df, time_period, plot) {

	# find col index for that candidate's name
	cand_indx <- which(colnames(polling_df) == candidate)

	# subset df to last 21 days, ignore last 2 days (no 2016 polling available)
  polling_df <- subset(polling_df, days_to_caucus < 21 & days_to_caucus > 0)

	# check if there are any rows that aren't NA
	if(all(is.na(polling_df[,cand_indx]))) {
		return(0)
	} else {

		# simple linear model
		lm_model <- lm(polling_df[,cand_indx] ~ days_to_caucus, data=polling_df)

		if(plot == TRUE) {

			# plot data
			plot(x=polling_df$days_to_caucus,
				y=polling_df[,cand_indx],
				# ylim=c(0, results_df[results_df$Candidate==candidate, VOTE_SHARE_COL] + 10),
				xlim=c(max(na.omit(polling_df$days_to_caucus)), min(na.omit(polling_df$days_to_caucus))),
				xlab="Days to Caucus",
				ylab="Vote Share",
				#main=toupper(candidate),
        main="Simple Linear")

			# add regression line
			abline(lm_model,
				col="red",
				lwd="2")

			# add actual results line
			abline(h=results_df[results_df$candidate==candidate, VOTE_SHARE_COL],
				lwd=2,
        col="green")
		}

		# predict vote share on election day
		election_day_df <- data.frame(c(candidate),c("0"))
		colnames(election_day_df) <- c("candidate", "days_to_caucus")
		election_day_df$days_to_caucus <- as.numeric(as.character(election_day_df$days_to_caucus))

		pred_vote_share <- predict(lm_model, newdata=election_day_df)
		
		return(pred_vote_share)
	}
}


complexLinearExtrapolateCandidate <- function(candidate, polling_df, results_df, time_period, plot) {

	# find col index for that candidate's name
	cand_indx <- which(colnames(polling_df) == candidate)
  
  # subset df to last 21 days, ignore last 2 days (no 2016 polling available)
  polling_df <- subset(polling_df, days_to_caucus < 21 & days_to_caucus > 0)

	# check if there are any rows that aren't NA
	if (all(is.na(polling_df[,cand_indx]))) {
		return(0)
	} else {

		# simple linear model
		lm_model <- lm(polling_df[,cand_indx] ~ days_to_caucus + I(days_to_caucus^2) + I(days_to_caucus^3), data=polling_df)

		if(plot == TRUE) {

			# print model
			# print(summary(lm_model))

			# plot data
			plot(x=polling_df$days_to_caucus,
				y=polling_df[,cand_indx],
				# ylim=c(0, results_df[results_df$Candidate==candidate, VOTE_SHARE_COL] + 10),
				xlim=c(max(na.omit(polling_df$days_to_caucus)), min(na.omit(polling_df$days_to_caucus))),
				xlab="Days to Caucus",
				ylab="Vote Share",
		#		main=toupper(candidate),
        main="Complex Linear")

			# add regression line
			lines(x=polling_df$days_to_caucus, 
				  y=predict(lm_model), type='l', col="red", lwd=2)


			# add actual results line
			abline(h=results_df[results_df$candidate==candidate, VOTE_SHARE_COL],
				lwd=2,
        col="green")
		}

		# predict vote share on election day
		election_day_df <- data.frame(c(candidate),c("0"))
		colnames(election_day_df) <- c("candidate", "days_to_caucus")
		election_day_df$days_to_caucus <- as.numeric(as.character(election_day_df$days_to_caucus))

		pred_vote_share <- predict(lm_model, newdata=election_day_df)
		
		return(pred_vote_share)
	}
}


#### EPANECHNIKOV

# chooseEpanechnikovBandwidthTrendline <- function(eplog_mse) {
# 	lowest_mse_indx <- which(eplog_mse$mse == min(eplog_mse$mse))
# 	lowest_mse_bw <- eplog_mse$bw_value[lowest_mse_indx]
# 	return(lowest_mse_indx)
# }

epanechnikovExtrapolateCandidate <- function(candidate_name, year_scope, results_df, time_period, plot) {

	# extract estimated vote share
	pred_vote_share <- subset(ep_preds, year==year_scope[1] & scope==year_scope[2] & bw_value==EP_BW & trendline_width==EP_TRENDLINE & candidate==candidate_name, select=estimate)

	return(pred_vote_share[1,1])
}

# epanechnikovExtrapolateCandidate("mccain", c("2008", "national"), results2008, FALSE)

# predCandidates
# --------------
# Given a list of candidates, polling data, and a model, generate predictions for each candidate
# returns df with candidate names and predictions
predCandidates <- function(polling_df, results_df, model, time_period) {

	# init df to store predictions
	pred_votes <- data.frame(results_df$candidate, c(NA))
	colnames(pred_votes) <- c('candidate', 'vote_share')

	for (cand in results_df$candidate) {

			pred_votes$vote_share[pred_votes$candidate == cand] <- model(cand, polling_df, results_df, time_period, FALSE)
	}
	return(pred_votes)
}


# calcRMSE
# --------
# Given a df of prediction and a df with actual results, compute the rmse for the predictions
calcRMSE <- function(pred_votes, results_df) {

	# order both dfs by candidate name, just to be safe
	pred_votes <- pred_votes[order(pred_votes$candidate),]
	results_df <- results_df[order(results_df$candidate),]

	mse <- sum((results_df$percentage - pred_votes$vote_share)^2) / nrow(pred_votes)
	rmse <- sqrt(mse)
	return(rmse)
}

# calcMSE
# --------
# Given a df of prediction and a df with actual results, compute the rmse for the predictions
calcMSE <- function(pred_votes, results_df) {

	# order both dfs by candidate name, just to be safe
	pred_votes <- pred_votes[order(pred_votes$candidate),]
	results_df <- results_df[order(results_df$candidate),]

	mse <- sum((results_df$percentage - pred_votes$vote_share)^2) / nrow(pred_votes)
	return(mse)
}



# testPollTrendsModel
# -------------------
# Given polling and results for an election, see how well a specified model extrapolates from polling data
testPollTrendsModel <- function(polling_df, results_df, model, time_period) {

	pred_votes <- predCandidates(polling_df, results_df, model, time_period)
	rmse <- calcMSE(pred_votes, results_df)
	return(rmse)
}

mse_simpleLinear <- (testPollTrendsModel(polls2008_iowa, results2008, linearExtrapolateCandidate, 0) + testPollTrendsModel(polls2012_iowa, results2012, linearExtrapolateCandidate, 0)) / 2

mse_complexLinear <- (testPollTrendsModel(polls2008_iowa, results2008, complexLinearExtrapolateCandidate, 0) + testPollTrendsModel(polls2012_iowa, results2012, complexLinearExtrapolateCandidate, 0)) / 2


## see what time period produces the lowest MSE for simple polling averages
chooseAverageInterval <- function(polling_df, results_df) {
	avg_days_test <- vector(length=21)

	for (i in 1:28) {
		avg_days_test[i] <- testPollTrendsModel(polling_df, results_df, averageLatestPollingCandidate, i)
	}

	return(
		which(avg_days_test == min(avg_days_test, na.rm =TRUE))
	)
}

AVG_INTERVAL <- chooseAverageInterval(polls2008_iowa, results2008) # 4 in 2008, 3 in 2012, go with 4


##############
# BUILD & TEST COMBINED MODEL OPTIONS
##############

predCandidates(polls2008_national, results2008, averageLatestPollingCandidate, AVG_INTERVAL)

# buildPredictionMatrix
# ---------------------
# Given polling and results dfs, construct a matrix of observations to use in regression models
buildPredictionMatrix <- function(state_polling_df, natl_polling_df, year, scope, results_df) {

  # construct vector of predictor names
	pred_names <- c('lm_1_iowa', 'lm_2_iowa', 'lm_1_natl', 'ep_iowa', 'ep_natl', 'avg_iowa')

	# build matrix of predictors
	preds <- matrix(nrow=nrow(results_df), ncol=length(pred_names))
	rownames(preds) <- results_df$candidate

	# add polling model predictions
	preds[,1] <- predCandidates(state_polling_df, results_df, linearExtrapolateCandidate, AVG_INTERVAL)$vote_share
	preds[,2] <- predCandidates(state_polling_df, results_df, complexLinearExtrapolateCandidate, AVG_INTERVAL)$vote_share
	preds[,3] <- predCandidates(natl_polling_df, results_df, linearExtrapolateCandidate, AVG_INTERVAL)$vote_share
	#preds[,3] <- 0 # ultimately didn't use linear national vote share
	preds[,4] <- predCandidates(c(year, scope), results_df, epanechnikovExtrapolateCandidate, AVG_INTERVAL)$vote_share
	preds[,5] <- predCandidates(c(year, "national"), results_df, epanechnikovExtrapolateCandidate, AVG_INTERVAL)$vote_share
	preds[,6] <- predCandidates(state_polling_df, results_df, averageLatestPollingCandidate, AVG_INTERVAL)$vote_share
	# preds[,6] <- 0


	# add predictor names to matrix
	colnames(preds) <- pred_names

	# replace any NAs with zeros
	preds[is.na(preds)] <- 0

	return(preds)
}


# scaleToVoteShares
# ------------------------------------
# Assuming the predictions from our model don't add up to 100, scale them proportionally: voteshare[candidate] / sum(voteshare) * 100
scaleToVoteShares <- function(raw_preds) {
	scaled_preds <- (raw_preds / sum(raw_preds)) * 100
	return(scaled_preds)
}


# scaleSortResults
# ----------------
# Scale and sort predictions
scaleSortResults <- function(preds) {
	scaled_preds <- scaleToVoteShares(preds)
	sorted_preds <- scaled_preds[order(scaled_preds[,1], decreasing=TRUE),]
	df_preds <- data.frame(sorted_preds)
	return(df_preds)
}


# combinePredictorsIntoModel
# --------------------------
# Using the outputs from predictFromPollTrends and predictFromLatestPolling outputs for both Iowa and national polling, build a model applying these predictors to 2008 and 2012 data. See which combinations work best with LOOCV.
# Return df/matrix of candidate names (rows) and predicted vote shares from different models (cols)

combinePredictorsIntoModel <- function(state_polling_df, natl_polling_df, results_df, combined) {

	########### BUILD MATRICES ###########

	# extract year and state from state_polling_df name
	polling_df_str <- deparse(substitute(state_polling_df))
	# strip out "polls"
	year_scope_str <- sub("polls", "", polling_df_str)
	# create list with year and scope
	year_scope <- strsplit(year_scope_str, "_", fixed=TRUE)
	year <- year_scope[[1]][1]
	scope <- year_scope[[1]][2]

	if (combined) {

		# create matrices to use in models
		preds_2008 <- buildPredictionMatrix(polls2008_iowa, polls2008_national, "2008", "iowa", results2008)
		preds_2012 <- buildPredictionMatrix(polls2012_iowa, polls2012_national, "2012", "iowa", results2012)

		# combine into overall matrix
		preds_train <- rbind(preds_2008, preds_2012)
		results_train <- rbind(results2008, results2012)

		print(preds_train)

	} else {

		# create matrix to use in models
		preds_train <- buildPredictionMatrix(state_polling_df, natl_polling_df, year, scope, results_df)
		results_train <- results_df
	}

	# build matrix to predict 2016
	preds_2016 <- buildPredictionMatrix(polls2016_iowa, polls2016_national, "2016", "iowa", candidates_2016)

	print(preds_2016)


	########### BUILD MODELS ###########
	
	# run simple linear regression 
	linear_model <- lm(results_train$percentage ~ ., data=as.data.frame(preds_train))
	print("############## LINEAR ##############")
	print(summary(linear_model))
	print(predict(linear_model, newdata = as.data.frame(preds_2016)))

	# run LASSO on predictors
	lasso <- glmnet(x = preds_train, y = results_train$percentage)

	# cross validate lasso
	lasso_cv<- cv.glmnet(x = preds_train, y = results_train$percentage, nfolds = nrow(preds_train))
	# plot(lasso_cv)

	# check RMSE
	lasso_train_preds <- predict(lasso, newx=preds_train, s = lasso_cv$lambda.min )
	print("Training RMSE:")
	print(rmse(results_train$percentage, lasso_train_preds))

	# print out final model chosen by lasso
	lasso_lowestMSE_index <- which(lasso$lambda == lasso_cv$lambda.min)
	print("############## LASSO BETAS ##############")
	print(lasso$beta[,lasso_lowestMSE_index])



	########### PREDICT 2016 ###########

	# predict 2016 using lasso
	lasso_preds <- predict(lasso, newx=preds_2016, s = lasso_cv$lambda.min )

	# scale and sort results
	df_preds_2016 <- scaleSortResults(lasso_preds)
  
  ## list of things to return
  return_list <- list(df_preds_2016, lasso, lasso_cv)

	# return(df_preds_2016)
  
  return(return_list)

	# return(lasso)

	# return(linear_model)
}

model_info <- combinePredictorsIntoModel(polls2012_iowa, polls2012_national, results2012, TRUE)

df_preds_2016 <- model_info[[1]]
lasso <- model_info[[2]]
lasso_cv <- model_info[[3]]

print(df_preds_2016)

```

# Predictions

__We predict XXX to be the winner of the 2016 Iowa Republican caucus.__

| Candidate | Vote Share |
|:------|:-----|
| Donald Trump  | 0 |
| Ted Cruz  | 0 |
| Marco Rubio  | 0 |
| Jeb Bush | 0 |
| Ben Carson | 0 |
| Chris Christie | 0 |
| Rand Paul | 0 |
| Mike Huckabee | 0 |
| John Kasich |  0 |

# Methodology

## Key Feature: Iowa Polling Trends

We focused our attention on in-state polling leading up to the caucus, since this has historically been the best indicator of a candidates' standing in Iowa. We used polls aggregated for Iowa by pollster.com (which was later acqured by _The Huffington Post_) for [2008](http://www.pollster.com/polls/ia/08-ia-rep-pres-primary.html) and [2012](http://elections.huffingtonpost.com/pollster/2012-iowa-gop-primary.csv). (All of the code used in this project is available at: [https://github.com/jeffbarrera/iowa-caucus/](https://github.com/jeffbarrera/iowa-caucus/) ). We wrote python scripts ([2008](https://github.com/jeffbarrera/iowa-caucus/blob/master/pollster/clean_2008_data.py) ; [2012/16](https://github.com/jeffbarrera/iowa-caucus/blob/master/pollster/clean_2012_2016_data.py) ) to standardize the data across years and add one key variable: the number of days before the Iowa Caucus is held.

Our core challenge with this polling data is twofold: First, to calculate a polling average that's accurate as of the present; and second, to use our knowledge of the present to estimate the final caucus vote totals. 

We tested a number of approaches for finding a polling average by looking at 2008 and 2012 polling data (which goes all the way to caucus): linear regression, lowess regression and non-parametric regression with gaussian and epanechnekov kernels.

```{r, echo=FALSE, results="hide", fig.height=4}

plotExtrapolateCandidate <- function(candidate, polling_df, results_df, time_period, plot) {

  # find col index for that candidate's name
	cand_indx <- which(colnames(polling_df) == candidate)

	# check if there are any rows that aren't NA
	if (all(is.na(polling_df[,cand_indx]))) {
		print(candidate)
		print("All observations were NA")
		return(0)
	} else {

		if(plot == TRUE) {

			# print model
			# print(summary(lm_model))
      
      # subset df to last 50 days
      polling_df <- subset(polling_df, days_to_caucus < 50)

			# plot data
			plot(x=polling_df$days_to_caucus,
				y=polling_df[,cand_indx],
				col="grey",
				# ylim=c(0, results_df[results_df$Candidate==candidate, VOTE_SHARE_COL] + 10),
				xlim=c(max(na.omit(polling_df$days_to_caucus)), min(na.omit(polling_df$days_to_caucus))),
				xlab="Days to Caucus",
				ylab="Vote Share",
				main="Trendline Comparison")

			# simple linear model
			lm_model <- lm(polling_df[,cand_indx] ~ days_to_caucus, data=polling_df)

			# add simple regression line
			abline(lm_model,
				col="red",
				lwd="2")

			# add complex regression line
			lm_model_complex <- lm(polling_df[,cand_indx] ~ days_to_caucus + I(days_to_caucus^2) + I(days_to_caucus^3), data=polling_df)
			
			# lines(x=polling_df$days_to_caucus, 
				  # y=predict(lm_model_complex), type='l', col="orange", lwd=2)


			# add lowess regression line
			cand_lowess = lowess(polling_df$days_to_caucus, polling_df[,cand_indx], f=.2)
			lines(cand_lowess, 
				col="blue",
				lwd="2")

			# add gaussian line
			gaussian_smooth = ksmooth(polling_df$days_to_caucus, polling_df[,cand_indx], "normal", bandwidth = 4)
			lines(gaussian_smooth,  col="orange", lwd=2)

			# add ep line
			cand_epan <- lpepa(polling_df$days_to_caucus, polling_df[,cand_indx], bandw=13, order=2, n.out=1000)
			lines(cand_epan$x.out, cand_epan$est,  col="purple", lwd="2")


			# add actual results line
			abline(h=results_df[results_df$candidate==candidate, VOTE_SHARE_COL],
				lwd=2,
        lty=2,
				col="black")
		}
	}
}

plotExtrapolateCandidate("romney", polls2008_iowa, results2008, 0, TRUE)

```

_Comparison of different extrapolation techniques applied to Mitt Romney in 2012: actual vote share in black, simple linear regression in red, lowess in blue, gaussian in orange, and epanechnekov in purple._

Testing scripts are here for [lowess](https://github.com/jeffbarrera/iowa-caucus/blob/master/test_lowess.py) ([results](https://github.com/jeffbarrera/iowa-caucus/blob/master/lowesslog_mse.csv));  [gaussian](https://github.com/jeffbarrera/iowa-caucus/blob/master/test_ksmooth_bandwidths.py) ([results](https://github.com/jeffbarrera/iowa-caucus/blob/master/kplog_mse.csv)) and [epanechnikov](https://github.com/jeffbarrera/iowa-caucus/blob/master/test_epanechnikov_bandwidth.py) ([results](https://github.com/jeffbarrera/iowa-caucus/blob/master/eplog_mse.csv)) kernels. We tested each approach with a variety of bandwidths (or f values in the case of lowess) and compared the final estimated point with the actual polling result. The most accurate estimation result in terms of overall MSE for 2008 and 2012 was obtained using an epanechnikov kernel with a bandwidth of 13 days, which gave us an MSE of [7.29](https://github.com/jeffbarrera/iowa-caucus/blob/master/eplog_mse.csv#L14). 

Arguably running this test naively--that is, just testing it on 2008 and 2012 without any cross-validation--could lead to a type of overfitting, but we'd expect the size of improvement to be minimal. We [noticed](https://github.com/jeffbarrera/iowa-caucus/blob/master/eplog_mse_cycle.csv) that MSE was minimized for the 2008 and 2012 cycle with a bandwith of 13 and 14 days respectively, which suggests there are similar dynamics (despite the races being vary different).

```{r, include=FALSE}

# load mse datasets to build table below
lowess_mse_df <- read.csv("lowesslog_mse.csv")
gaussian_mse_df <- read.csv("kplog_mse.csv")
ep_mse_df <- read.csv("eplog_mse.csv")

# find min values
lowess_min_mse <- min(lowess_mse_df$mse)
gaussian_min_mse <- min(gaussian_mse_df$mse)
ep_min_mse <- min(ep_mse_df$mse)

```

|  | Simple Linear Model | Complex Linear Model | Lowess | Gaussian | Epanechnikov |
|------:|:-----:|:-----:|:-----:|:-----:|:-----:|
| Lowest MSE | `r round(mse_simpleLinear, digits=2)` | `r round(mse_complexLinear, digits=2)` | `r round(lowess_min_mse, digits=2)` | `r round(gaussian_min_mse, digits=2)` | `r round(ep_min_mse, digits=2)` |

To answer the second half of the problem--how to interpolate from a polling average several days ahead of the caucus to a final vote share on caucus day, we assumed it would be best to start with the best polling average result. We tested a number of possible measures of the 'momentum' by drawing a line from the final average poll result and the result from several days earlier. Our testing suggested 14 days worked best. Again, this wasn't cross-validated in any way, but it sounds good--that's the last point outside of the epanechnikov window. We also assume that interpolation error isn't huge (Santorum 2012 aside).

```{r, echo=FALSE, fig.width=7, fig.height=3}

ep_mse_interp <- read.csv("ep_interp_mse_pred.csv", stringsAsFactors=FALSE)

plot(x=ep_mse_interp$trendline_width,
     y=ep_mse_interp$mse,
     type="l",
     xlab="Days to caucus",
     ylab="MSE",
     main="Choosing the period to interpolate over")

```

We then applied this model to generate a predicted vote share for each candidate in the 2008, 2012, and 2016 caucuses.

### Iowa Polling Average
In case our interpolated projections were worse than the unimproved polling average several days out, we also included as a feature the vote shares for each candidate in the days leading up to the election. We tested from 2 to 21 days out from the caucus, and calculated the RMSE for each interval. The lowest RMSE was at 4 days in 2008 and 3 days in 2012, so we went with the rounded average of 4 days.

## Features Not Included

We considered a number of additional features, but chose not to include them for various reasons:

### National Polling Trends
In their ["polls-plus" model](http://fivethirtyeight.com/features/how-we-are-forecasting-the-2016-presidential-primary-election/), 538 uses national polling as a contrarian indicator, based on [data suggesting that candidates who poll better in a particular state than they do nationally tend to do better than their statewide polls](http://fivethirtyeight.com/features/to-win-in-iowa-or-new-hampshire-it-may-be-better-to-poll-worse-nationally/). We tried a similar approach, applying the epanechnikov technique we used to estimate statewide polling trends to national polling for each candidate. However, we ultimately decided that since our training data didn't have a comparable case of a celebrity frontrunner, including this predictor skewed our results -- Trump was likely being over-penalized for his high national poll numbers. 

### Campaign Finance Data
Reports filed with the Federal Elections Commission give some insight into a candidates' fundraising and spending, but we didn't make analysis of those a priority  for several reasons:

- This year is different! One candidate (guess who) has been the beneficiary of millions in "earned media" &mdash; coverage that's not paid for. 

- Candidates' reports are filed at a significant lag. Quarterly reports covering the fourth quarter of 2015 are due Jan. 31, but do not reflect any spending or fundraising that took place in 2016. 

- The 2012 and 2008 Iowa caucuses were held Jan. 2 (two days after the end of a filing period) whereas the 2016 caucuses are held Feb. 1 (a month after the end of the most recently available candidate spending data). Thus a relationship between financial figures for 2008 and 2012 wouldn't necessarily hold true for 2016. 

-  The way that campaigns spend money is in flux and increasingly money spent is excluded from public accounting. 

### Endorsements
Fivethirtyeight uses a weighted endorsements system to help predict primary results. What their data [show](http://projects.fivethirtyeight.com/2016-endorsement-primary/), this year, however, is that there are far fewer endorsements this year than in previous cycles. The outsider nature of several candidates, and their relative paucity of endorsements to date, also makes us skeptical of this measure.

### Crosstabs available in polls
Most reputable polls provide results cross-tabulated by various demographic groups. Unfortunately, we were unable to find any easily available aggregation of poll crosstabs (and the inconsistent approach pollsters take would make this a considerable challenge). Nonetheless, we believe this might be a useful indicator. Were this data available in bulk we might be able to make different assumptions about the electorate. Data suggest many potential voters who say they plan to participate in caucuses do not actually do so; we believe voter subgroups' lie to pollsters at a differential rate, introducting a meaningful bias into polls. Careful consideration of prior years voter crosstabs and exit polls (confusing known as entrance polls in Iowa) might help.

## Model

Once we had these predictors, we used a LASSO regression to determine how to weight each feature. Treating the actual vote share for each candidate in 2008 and 2012 as our training Y variable, we calculated $\beta$ coefficients for each feature at different values of $\lambda$. We then used leave-one-out cross-validation to see how each model performed out-of-sample. We selected the model with the lowest out-of-sample MSE, in the hope that this would be predictive in 2016 but would also avoid overfitting to the 2008 and 2012 data. Ultimately, this approach led us to use a single predictor: .94 times the epanechnikov trendline estimate for election day.

```{r fig.align='right', echo=FALSE}

library(knitr)

# extract coefficients from LASSO
lasso_lowestMSE_index <- which(lasso$lambda == lasso_cv$lambda.min)
# round(lasso$beta[,lasso_lowestMSE_index], digits=2)

# kable(lasso$beta[,lasso_lowestMSE_index])

```

 We then used this model to predict vote shares for each of the 2016 candidates. Finally, since the sum of these predictions likely would not exactly equal 100%, we scaled them proportionally to produce our estimates of vote share.

## Limitations & Opportunities for Improvement

The biggest limitation of our model is that it does not include any features intended to measure voter turnout or the "ground game" &mdash; the campaigns' efforts to identify supporters and get them to show up to caucus. Historically, this has been a critical aspect of winning in Iowa: since caucuses typically have [low turnout rates](http://iowacaucusproject.org/2015/07/how-many-people-participate-in-the-iowa-caucuses/), effectively mobilizing supporters can have a big impact on a candidate's vote share. However, we were unable to find a good way to measure organizing operations or predict turnout, since we couldn't obtain useful campaign finance or crosstab data.

Ultimately, we had to rely on assumptions about the relationship between polling and turnout. All the polls we're aggregating are of "likely voters," and some of the the polling firms also weight their responses based on estimated turnout models. We thus hope that the relationship between these polls of likely voters and the final vote share is reasonably consistent across elections, and that the coefficients in our LASSO model will capture this relationship.

If this relationship is not consistent, however (perhaps because supporters of "outsider" candidates like Trump may be more likely to lie to pollsters about whether they will caucus), this could throw off our model. Given more time and detailed cross-tabulated data, it may be possible to explore and account for these differences, and produce more nuanced models of the relationship between a candidate's poll numbers, turnout rates, and actual vote share. This could be an interesting avenue to explore in the future.